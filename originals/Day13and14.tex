% ==============================================================================



%% the first is for standalone compiling,
%%   the second looks for the master document

%\documentclass[classnotes]{fillsntsx}
%\documentclass[../../../Master/AppliedStochastics.tex]{subfiles}






%\include{../../../../macros}

\usepackage[margin=1in]{geometry} %For margins 
















% ==============================================================================


%% without the master file this macro does nothing
%\course{Applied Stochastic Processes}


%\author{Helen}  % your name
%\date{22 and 24 October 2018}    % the date of the lecture


%% any custom macros should go here, but please be conservative

\DeclareMathOperator{\PPP}{PPP}




% ==============================================================================
%
\begin{document}
%
% ==============================================================================


%\makelecture % this is effectively \maketitle, the lecture follows


% Day 13

\section{Examples}

\begin{exmp}{Motivating example: Moments of the Poisson}

Let $(N_t)_{t \geq 0}$ be a Poisson counting process where $N \sim \PPP(\text{constant rate 1})$ on $[0, \infty)$. 
The fact that  $(N_t)_{t \geq 0}$ is a Poisson counting process means that
$N_t \in \Z$ and $N_t$ ``jumps by 1 unit at rate 1'', i.e. $N_t = N([0, t]) + N_0$. 
Note that writing $N \sim \PPP(\text{constant rate 1})$ is the same as writing $N \sim \PPP(\lambda)$, where $\lambda$
is the Lebesgue measure. 

We want to answer the question: What is $\E[N_t^k]$? First we define
$$\begin{aligned}
\E^x[f(N_t)] := \E[f(N_t) | N_0 = x ] 
\end{aligned}$$
We see that
\begin{align}
\E^x[f(N_t)] = f(x) + \int_0^t \frac{d}{ds} \E^x [f(N_s)] ds &= f(x) + \int_0^t  \lim_{\epsilon \to 0} \frac{1}{\epsilon} \E^x [f(N_{s+\epsilon}) - f(N_s)] ds\\
\label{condexp1}
&=  f(x) + \int_0^t  \lim_{\epsilon \to 0}   \frac{1}{\epsilon} \E^x [ \E^s [f(N_{s+\epsilon}) - f(N_s) | N_s] ] ds
\end{align}
This motivates the following definition.
\begin{defn} Let
$$\begin{aligned}
(Gf)(y) := \lim_{\epsilon \to 0} \frac{1}{\epsilon} \E^{y}[ f(N_\epsilon) - f(y) ] 
\end{aligned}$$
$G$ is called the {\em generator} for $N_t$. 
\end{defn}


Since $N_\epsilon - N_0 \sim \Poisson(\epsilon)$, 
$$\begin{aligned}
Gf(x) &= \lim_{\epsilon \to 0} \frac{1}{\epsilon} \E^{x} [f(N_\epsilon) - f(x)] \\
& =   \lim_{\epsilon \to 0} \frac{1}{\epsilon} \E[f(N_\epsilon) - f(x) | N_0 = x] \\
& =   \lim_{\epsilon \to 0}  \frac{1}{\epsilon} ( \P \{ N_\epsilon - N_0 = 0 \} ( f(x) - f(x) ) +
\P \{ N_\epsilon - N_0 = 1 \} ( f(x+1) - f(x) )  \\
 & + \P \{ N_\epsilon - N_0 \geq 2 \} \E[f(N_\epsilon) - f(x) | N_\epsilon - N_0 \geq 2 ] )\\
& =   \lim_{\epsilon \to 0}  \frac{1}{\epsilon}  \left( e^{-\epsilon} \cdot \epsilon \cdot (f(x+1) - f(x)) + \sum_{n \geq 2} \dfrac{e^{-\epsilon} \epsilon^n}{n!}\E[f(N_\epsilon) - f(x) | N_\epsilon - N_0 \geq 2 ]  \right) \\
& =   \lim_{\epsilon \to 0}  \frac{1}{\epsilon}  \left( e^{-\epsilon} \cdot \epsilon \cdot (f(x+1) - f(x)) + \mathcal{O}(\epsilon^2) \right) \\
& =  f(x+1) - f(x)
 \end{aligned}$$
 So $Gf(x) = f(x + 1) - f(x)$ is the generator for the constant rate 1 Poisson process. 
 
 By the Markov property, 
$$\begin{aligned}
 \lim_{\epsilon \to 0}   \frac{1}{\epsilon} \E^x [ \E^s [f(N_{s+\epsilon}) - f(N_s) | N_s] ] =  \E^x[Gf(N_s)]
 \end{aligned}$$
It follows from this and (\ref{condexp1}) that
 \begin{align}
 \label{condexp2}
 \E^x[f(N_t)] = f(x) + \int_0^t \E^x [f(N_s + 1) - f(N_s) ] ds
 \end{align}
 
Next we need the following notation. Let 
$$\begin{aligned}
(x)_m := x(x-1) \cdots (x-m+1)
\end{aligned}$$

Observe that 
$$\begin{aligned}
(x+1)_m - (x)_m &= (x+1) x (x-1) \cdots (x-m+2) - x (x-1) \cdots (x-m+1)\\
& = ( (x+1) - (x-m+1) ) (x)_{m-1} \\
&= m(x)_{m-1}
\end{aligned}$$

Now by (\ref{condexp2}),
$$\begin{aligned}
\E^0 [ (N_t)_m ] =0 + \int_0^{t}  \E^{0} [(N_s+1)_m - (N_s)_{m}] ds = 0 + \int_0^{t} m \E^{0} [(N_s)_{m-1}] ds 
\end{aligned}$$
and $\E^0 [ (N_t)_0 ] = 1$. 

Let 
$$\begin{aligned}
g_{m}(t) := \int_0^{t} m \E^{0} [(N_s)_{m-1}] ds
\end{aligned}$$ 
and let $g_0(t) = 1$. 
Then $g_{m}(t) = \int_0^{t} m g_{m-1}(s) ds$, so
$$\begin{aligned}
g_1(t) &=  \E^0 [N_t]=  \int_{0}^{t}  1ds = t \\
g_2(t) &= \E^0 [N_t(N_t - 1)]  =  \int_{0}^{t}  2sds = t^2 \\
\end{aligned}$$
and by induction,
$$\begin{aligned}
  \E^0 [(N_t)_m] = g_m(t) = t^m
  \end{aligned}$$
\end{exmp}

\begin{exmp}{What is the generator for Brownian motion?}

Let $(B_t)_{t \geq 0}$ be a Brownian motion, $B_{t+s} - B_t \sim \Normal(0, s)$. 
Then
$$\begin{aligned}
\E^x[f(B_\epsilon)] &\approx \E^x[ f(x) + f'(x) (B_\epsilon - x) + \frac{1}{2}f''(x)(B_\epsilon - x)^2 + \cdots] \\
& =  f(x) + f'(x)\cdot 0 +  \frac{1}{2}f''(x) \cdot \epsilon + \mathcal{O}(\epsilon^{3/2})
\end{aligned}$$
So
$$\begin{aligned}
\lim_{\epsilon \to 0} \dfrac{1}{\epsilon} \E^{x}[ f(B_\epsilon) - f(x) ] = \dfrac{1}{2} f''(x)
\end{aligned}$$
and thus $Gf(x) = \dfrac{1}{2} f''(x)$ is the generator for standard Brownian motion. 

\end{exmp}

\begin{exmp}{How can we make $N_t$ into Brownian motion?}
We know that:
\begin{enumerate}
\item By the Central Limit Theorem, adding up independent noise, centering and scaling gets you the Gaussian distribution. 
\item $N_t$ is the number of points in a PPP on $[0, t]$, and so is a sum of a bunch of independent things. 
\item Brownian motion started at zero has $\E[B_t] = 0$, $\E[B_t^2] = t$.
\end{enumerate}

$N_t$ does not have enough noise in each interval to be Brownian. So consider $N_{Mt}$. Since
$\E[N_{Mt}] = Mt$, we subtract $Mt$, and then divide by $\sqrt{M}$ to get the correct variance.
Thus we define
$$\begin{aligned}
X_t^{(M)} = \dfrac{ N_{Mt} - Mt}{\sqrt{M}}
\end{aligned}$$

Let $G_M$ denote the generator of $X_t^{(M)}$. Then
$$\begin{aligned}
G_M f(x) = M \left( f \left( x+ \frac{1}{\sqrt{M}}\right) - f(x) \right) - \dfrac{1}{\sqrt{M}} f'(x)
\end{aligned}$$
This is discussed in more detail on Day 15. 
As $M \to \infty$, $G_M f(x) \to \dfrac{1}{2} f''(x)$. 
Therefore, 
$$\begin{aligned}
(X^{(M)})_{t \geq 0} \xrightarrow[M \to \infty]{d} (B_t)_{t \geq 0}
\end{aligned}$$
\end{exmp}


% Day 14

\section{Theory}

Let $(X_t)_{t \geq 0}$ be a time-homogeneous Markov process on a locally compact, separable metric space $S$, 
and define
$C_0 := C_0(S)$ to be the set of all continuous functions $f: S \to \R$ such that $f(x) \to 0$ as $|x| \to \infty$. 
Note that $C_0$ is a Banach space with norm $|| \cdot ||_{\infty}$. Assume $\P(\{X_t \in S \}) = 1$ for all $t$. 

\begin{defn}
Define the transition semigroup $(P_t)_{t \geq 0}$ by $(P_t f)(x) = \E^{x}[f(X_t)]$ for $f \in C_0$. 
\end{defn}

\begin{note}
The assumption
\begin{align}
\label{Feller1}
(X_t | X_0 = x) \xrightarrow[x \to y]{d} (X_t | X_0 =y) 
\end{align}
implies $P_t: C_0 \to C_0$.
\end{note}

We have the following properties: 
\begin{enumerate}
\item[(1)] $P_0 = I$ since $P_0 f(x) = \E^{x} [f(X_0)] = f(x)$. 
\item[(2)] $P_s P_t = P_{s + t}$
since
$$\begin{aligned}
P_s P_t f(x) = \E^{x}[P_t f(x_s) ] = \E^{x} [ \E^{x_s} [ f(x_t) ] ] 
= \E^{x} [ f(x_{t+s})]  
=  P_{s+t} f(x)
\end{aligned}$$
If we assume that
\begin{align}
\label{Feller2}
(X_t | X_0 = x) \xrightarrow[t \to 0]{d} x
\end{align}
for each $x$ then
\item[(3)] $P_t \to I$ as $t \to 0$. 
\end{enumerate}

\begin{defn}
The generator of $(X_t)_{t \geq 0}$ and/or $(P_t)_{t \geq 0}$ is
$$\begin{aligned}
G = \lim_{\epsilon \to 0} \dfrac{1}{\epsilon} (P_{\epsilon} - I)
\end{aligned}$$
and 
\begin{align}
\label{pt}
P_t = e^{tG} = \sum_{n \geq 0} \dfrac{t^{n}}{n!} G^n
\end{align}
if the above statements make sense. 
\end{defn}

\begin{note}
If $(X_t)_{t \geq 0}$ satisfies (\ref{Feller1}) and (\ref{Feller2}) it is {\em Feller}.
The generator of a Feller process uniquely determines its distribution. This is clear from (\ref{pt})
when (\ref{pt}) makes sense. To prove this claim in general, use resolvents. 
\end{note}
The generator has the following properties:
\begin{enumerate}
\item[(1)] $G1 = 0$ since $(P_t - I)1(x) = \E^{x}[1 - 1] = 0$
\item[(2)] $\pi$ is a stationary measure for $(X_t)_{t \geq 0}$ if and only if 
$$\begin{aligned}
\int G f(x) \pi (dx) = 0
\end{aligned}$$
for all $f$. 
\end{enumerate}
Next we consider some examples. 
\begin{exmp}
Let $Gf(x) = f'(x)$. Then 
$$\begin{aligned}
P_t f(x) =  \sum_{n \geq 0} \dfrac{t^{n}}{n!} f^{(n)}(x) =  \sum_{n \geq 0} \dfrac{t^{n}}{n!} f^{(n)}(x + t - t) =  f(x+t)
\end{aligned}$$
So $X_t = X_0 + t$. Therefore, $\dfrac{d}{dx}$ corresponds to ``deterministic flow at rate 1.''
\end{exmp}

\begin{exmp}
Let $Gf(x) = \dfrac{1}{2} f''(x)$. Recall that this is the generator for Brownian motion. 
Then 
$$\begin{aligned}
P_t f(x) =  \sum_{n \geq 0} \dfrac{2^{-n}t^{n}}{n!} f^{(2n)}(x) 
\end{aligned}$$
Let $\widehat{f}(\xi)$ be the Fourier transform of $f$. Then
$$\begin{aligned}
\widehat{P_t f}(x) =  \sum_{n \geq 0} \dfrac{2^{-n}t^{n}}{n!} (-\xi^2)^{n} \hat{f}(\xi) = e^{-\frac{t}{2} \xi^2} \widehat{f}(\xi)
\end{aligned}$$
Because $e^{-\frac{t}{2} \xi^2}$ is the Fourier transform of the Gaussian density with variance $t$, and the Fourier transform takes convolution to multiplication,
$$\begin{aligned}
P_t f(x) =  \sum_{n \geq 0} \dfrac{2^{-n}t^{n}}{n!} f^{(2n)}(x) 
= 
\int_{-\infty}^{\infty} \dfrac{1}{\sqrt{ 2 \pi t }} e^{-\dfrac{ (y-x)^2}{2t}} f(y) dy
\end{aligned}$$
\end{exmp}

It turns out that if $Gf(x) = f^{(k)}(x)$ for $k > 2$, $G$ is not the generator of a Feller process.
One reason for this is that for $k > 2$ there is no way to write a discrete approximation to $f^{(k)}(x)$ as a sum over values of $f$ with all coefficients except that of $f(x)$ positive. More generally, we can appeal to the following theorem.

\begin{thm}[Hille-Yosida Theorem]
Let $A$ be a linear operator on $\mathcal{D} \subseteq C_0$.
Then $A$ has closure that is the generator of a Feller process if and only if
\begin{enumerate}
\item[(i)] $\mathcal{D}$ is dense in $C_0$
\item[(ii)] The range of $\lambda - A$ is dense in $C_0$ for some $\lambda > 0$. 
\item[(iii)] (Positive Maximum Principle) If $f(x) \leq f(x_0)$ for all $x \in S$ and $f(x_0) > 0$, $f \in C_0$, then $A f(x_0) \leq 0$.
\end{enumerate}
\end{thm}

\begin{note}
If (i)-(iii) hold, then $(\lambda - A)^{-1}$ exists for all $\lambda > 0$. To see this, suppose that $\lambda - A$ is not invertible on $C_0$ for some $\lambda > 0$. 
Then there exists $f \in C_0$ such that $Af = \lambda f$. Then $\E^x[f(X_t)] =e^{t \lambda} f(x)$. But $\E^x[f(X_t)]$ is bounded since $f \in C_0$, so we have reached a contradiction. 
\end{note}

\begin{note}
If $A = \lim\limits_{t \to 0} \frac{1}{t} (P_t - I)$, then (iii) holds. For if
$$\begin{aligned}
A f(x_0) =  \lim\limits_{t \to 0} \frac{1}{t} (P_t - I) f(x_0) =  \lim_{t \to 0} \frac{1}{t} \E^{x_0} [f(x_t) - f(x_0)]
\end{aligned}$$
Since $f(x_t) - f(x_0) \leq 0$ by assumption, $\E^{x_0} [f(x_t) - f(x_0)] \leq 0$, so $A f(x_0) \leq 0$. 
\end{note}

% ==============================================================================
%
\end{document}
%
% ==============================================================================